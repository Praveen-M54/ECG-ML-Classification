{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This notebook downloads the entire MIT-BIH Arrhythmia Database, which is publicly available here: https://physionet.org/physiobank/database/mitdb/\n",
    "\n",
    "It then converts all the WFDB files into CSV files with the same name (example the files 100.dat, 100.hea, 100.atr into 100.csv).\n",
    "\n",
    "Each row in the CSV file corresponds to a single heartbeat and is generated as follows:\n",
    "* Split MIT-BIH record at the R-peaks into individual heartbeat records.\n",
    "* Each heartbeat record is appended with the first 40 readings of the next heartbeat record so that we include a full QRS Complex.\n",
    "* Resample each heartbeat record from 360Hz to 125Hz.\n",
    "* Normalize the mV readings to a 0-1 range.\n",
    "* Heartbeat records longer than 187 values are discarded.\n",
    "* Heartbeat records are padded with zeroes at the end until they contain exactly 187 values.\n",
    "* Heartbeat classifications from the annotations is reduced to just Normal and Abnormal and appended to the end of each heartbeat record (0 is normal, 1 is abnormal). Each row then contains exactly 188 values.\n",
    "* Heartbeat records without classifications are discarded.\n",
    "\n",
    "The purpose of these CSV files is so that they can be used in training the ECG model for classifying heartbeats as either Normal or Abnormal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize\n",
    "Import dependencies.\n",
    "\n",
    "Note that you will need to download and install the [mitdb](https://github.com/Nospoko/qrs-tutorial) library. The project contains convenience functions that make it easier to download and read [WFDB](https://physionet.org/physiotools/wfdb.shtml) compatible files. In addition, you will also need to install the [BioSPPy](https://github.com/PIA-Group/BioSPPy) library, which we use to find the R-peaks in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm\n",
    "%pip install wfdb\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb as wf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from datasets import mitdb as dm\n",
    "from biosppy.signals import ecg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Conversion\n",
    "Read the WFDB files and convert to CSV files. Data will be split into individual heartbeats, each row consisting of exactly 187 normalized and resampled values, plus the last value with be an integer representing the classification; 0 = Normal, 1 = Abnormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = dm.get_records()\n",
    "print('Total files: ', len(records))\n",
    "\n",
    "# Instead of using the annotations to find the beats, we will\n",
    "# use R-peak detection instead. The reason for this is so that\n",
    "# the same logic can be used to analyze new and un-annotated\n",
    "# ECG data. We use the annotations here only to classify the\n",
    "# beat as either Normal or Abnormal and to train the model.\n",
    "# Reference:\n",
    "# https://physionet.org/physiobank/database/html/mitdbdir/intro.htm\n",
    "realbeats = ['N','L','R','B','A','a','J','S','V','r',\n",
    "             'F','e','j','n','E','/','f','Q','?']\n",
    "\n",
    "# Loop through each input file. Each file contains one\n",
    "# record of ECG readings, sampled at 360 readings per\n",
    "# second.\n",
    "for path in records:\n",
    "    pathpts = path.split('/')\n",
    "    fn = pathpts[-1]\n",
    "    print('Loading file:', path)\n",
    "\n",
    "    # Read in the data\n",
    "    record = wf.rdsamp(path)\n",
    "    annotation = wf.rdann(path, 'atr')\n",
    "\n",
    "    # Print some meta informations\n",
    "    print('    Sampling frequency used for this record:', record[1].get('fs'))\n",
    "    print('    Shape of loaded data array:', record[0].shape)\n",
    "    print('    Number of loaded annotations:', len(annotation.num))\n",
    "    \n",
    "    # Get the ECG values from the file.\n",
    "    data = record[0].transpose()\n",
    "\n",
    "    # Generate the classifications based on the annotations.\n",
    "    # 0.0 = undetermined\n",
    "    # 1.0 = normal\n",
    "    # 2.0 = abnormal\n",
    "    cat = np.array(annotation.symbol)\n",
    "    rate = np.zeros_like(cat, dtype='float')\n",
    "    for catid, catval in enumerate(cat):\n",
    "        if (catval == 'N'):\n",
    "            rate[catid] = 1.0 # Normal\n",
    "        elif (catval in realbeats):\n",
    "            rate[catid] = 2.0 # Abnormal\n",
    "    rates = np.zeros_like(data[0], dtype='float')\n",
    "    rates[annotation.sample] = rate\n",
    "    \n",
    "    indices = np.arange(data[0].size, dtype='int')\n",
    "\n",
    "    # Process each channel separately (2 per input file).\n",
    "    for channelid, channel in enumerate(data):\n",
    "        chname = record[1].get('sig_name')[channelid]\n",
    "        print('    ECG channel type:', chname)\n",
    "        \n",
    "        # Find rpeaks in the ECG data. Most should match with\n",
    "        # the annotations.\n",
    "        out = ecg.ecg(signal=channel, sampling_rate=360, show=False)\n",
    "        rpeaks = np.zeros_like(channel, dtype='float')\n",
    "        rpeaks[out['rpeaks']] = 1.0\n",
    "        \n",
    "        beatstoremove = np.array([0])\n",
    "\n",
    "        # Split into individual heartbeats. For each heartbeat\n",
    "        # record, append classification (normal/abnormal).\n",
    "        beats = np.split(channel, out['rpeaks'])\n",
    "        for idx, idxval in enumerate(out['rpeaks']):\n",
    "            firstround = idx == 0\n",
    "            lastround = idx == len(beats) - 1\n",
    "\n",
    "            # Skip first and last beat.\n",
    "            if (firstround or lastround):\n",
    "                continue\n",
    "\n",
    "            # Get the classification value that is on\n",
    "            # or near the position of the rpeak index.\n",
    "            fromidx = 0 if idxval < 10 else idxval - 10\n",
    "            toidx = idxval + 10\n",
    "            catval = rates[fromidx:toidx].max()\n",
    "            \n",
    "            # Skip beat if there is no classification.\n",
    "            if (catval == 0.0):\n",
    "                beatstoremove = np.append(beatstoremove, idx)\n",
    "                continue\n",
    "\n",
    "            # Normal beat is now classified as 0.0 and abnormal is 1.0.\n",
    "            catval = catval - 1.0\n",
    "\n",
    "            # Append some extra readings from next beat.\n",
    "            beats[idx] = np.append(beats[idx], beats[idx+1][:40])\n",
    "\n",
    "            # Normalize the readings to a 0-1 range for ML purposes.\n",
    "            beats[idx] = (beats[idx] - beats[idx].min()) / beats[idx].ptp()\n",
    "\n",
    "            # Resample from 360Hz to 125Hz\n",
    "            newsize = int((beats[idx].size * 125 / 360) + 0.5)\n",
    "            beats[idx] = signal.resample(beats[idx], newsize)\n",
    "\n",
    "            # Skipping records that are too long.\n",
    "            if (beats[idx].size > 187):\n",
    "                beatstoremove = np.append(beatstoremove, idx)\n",
    "                continue\n",
    "\n",
    "            # Pad with zeroes.\n",
    "            zerocount = 187 - beats[idx].size\n",
    "            beats[idx] = np.pad(beats[idx], (0, zerocount), 'constant', constant_values=(0.0, 0.0))\n",
    "\n",
    "            # Append the classification to the beat data.\n",
    "            beats[idx] = np.append(beats[idx], catval)\n",
    "\n",
    "        beatstoremove = np.append(beatstoremove, len(beats)-1)\n",
    "\n",
    "        # Remove first and last beats and the ones without classification.\n",
    "        beats = np.delete(beats, beatstoremove)\n",
    "\n",
    "        # Save to CSV file.\n",
    "        savedata = np.array(list(beats[:]), dtype=np.float)\n",
    "        outfn = 'data_ecg/'+fn+'_'+chname+'.csv'\n",
    "        print('    Generating ', outfn)\n",
    "        with open(outfn, \"wb\") as fin:\n",
    "            np.savetxt(fin, savedata, delimiter=\",\", fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
